{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "celltoolbar": "Tags",
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Autism Brain disorder prediction using ABIDE dataset**"
      ],
      "metadata": {
        "id": "5NARKSl2ynte"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset introduction\n",
        "\n",
        "Autism Brain Imaging Data Exchange (ABIDE) initiative for providing functional and structural brain imaging datasets collected from several brain imaging centers around the world.\n",
        "\n",
        "Link of ABIDE: https://fcon_1000.projects.nitrc.org/indi/abide/abide_I.html\n"
      ],
      "metadata": {
        "id": "O5jyEx_v4RCw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation of the project\n",
        "\n",
        "a.\tApply a linear interpolation method called 'mixup' to enlarge the dataset. \\\n",
        "b.\tApply other AE models as feature extractors: Sparse AutoEncoder(SAE) and Variational AutoEncoder(VAE) \\\n",
        "c.\tConstruct a DNN model as the final classifier to improve the prediction accuracy.\n",
        "\n",
        "\n",
        "**The links of papers:**\\\n",
        "mixup: https://arxiv.org/abs/1710.09412 \\\n",
        "SAE: http://minegrado.ovh:3000/General-Team/docs/raw/commit/254ef7b55c2acb53284ec3bc85baa6432760cc4d/ML_DL_NN_books/papers/Autoencoders/sparseAutoencoder_2011new.pdf \\\n",
        "VAE: https://arxiv.org/abs/1312.6114 \\"
      ],
      "metadata": {
        "id": "RTwfzpQ55ZPm"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcZ3P2B2O3fL",
        "outputId": "bdf092eb-74ee-46be-d0b2-56333e48fcc1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGZwSG0lYbOG",
        "outputId": "3c14989b-70b4-4ba7-d8c1-485e4cc40d38"
      },
      "source": [
        "!pip install matplotlib\n",
        "!pip install pyprind"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyprind\n",
            "  Downloading PyPrind-2.11.3-py2.py3-none-any.whl (8.4 kB)\n",
            "Installing collected packages: pyprind\n",
            "Successfully installed pyprind-2.11.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeJBmZEN1nUC"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from functools import reduce\n",
        "from sklearn.impute import SimpleImputer\n",
        "import time\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import pyprind\n",
        "import sys\n",
        "import pickle\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from scipy import stats\n",
        "from sklearn import tree\n",
        "import functools\n",
        "import numpy.ma as ma # for masked arrays\n",
        "import pyprind\n",
        "import random\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from torch.autograd import Variable\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from typing import Tuple"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKvHHAjn1nUC"
      },
      "source": [
        "## Importing the data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhZCttRi1nUD"
      },
      "source": [
        "def get_key(filename):\n",
        "    f_split = filename.split('_')\n",
        "    if f_split[3] == 'rois':\n",
        "        key = '_'.join(f_split[0:3]) \n",
        "    else:\n",
        "        key = '_'.join(f_split[0:2])\n",
        "    return key"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Please exchange the path of dataset with your own.**"
      ],
      "metadata": {
        "id": "0ZW5-JZXxjzv"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQrBYT1u1nUD",
        "outputId": "cf45546b-6c5b-4044-e538-98cd1ceee166"
      },
      "source": [
        "data_main_path ='/content/drive/MyDrive/DL project/rois_cc200'#cc200'#path to time series data\n",
        "flist = os.listdir(data_main_path)\n",
        "print(len(flist))\n",
        "\n",
        "for f in range(len(flist)):\n",
        "    flist[f] = get_key(flist[f])\n",
        "\n",
        "df_labels = pd.read_csv('/content/drive/MyDrive/DL project/Phenotypic_V1_0b_preprocessed1.csv')#path \n",
        "\n",
        "df_labels.DX_GROUP = df_labels.DX_GROUP.map({1: 1, 2:0})\n",
        "print(len(df_labels))\n",
        "\n",
        "labels = {}\n",
        "for row in df_labels.iterrows():\n",
        "    file_id = row[1]['FILE_ID']\n",
        "    y_label = row[1]['DX_GROUP']\n",
        "    if file_id == 'no_filename':\n",
        "        continue\n",
        "    assert(file_id not in labels)\n",
        "    labels[file_id] = y_label"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1035\n",
            "1112\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gulzMbyL1nUE"
      },
      "source": [
        "### Helper functions for computing correlations\n",
        "\n",
        "The brain atlas is divided into 200 regions (the raw data of each sample is a time series of those regions). The first thing to do is generated feature vectors, finding those brain regions that highly connected based on the Pearson correlation coefficient. Some helper functions is defined below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Y9bTgDV1nUE"
      },
      "source": [
        "def get_label(filename):\n",
        "    assert (filename in labels)\n",
        "    return labels[filename]\n",
        "\n",
        "\n",
        "def get_corr_data(filename):\n",
        "    #print(filename)\n",
        "    for file in os.listdir(data_main_path):\n",
        "        if file.startswith(filename):\n",
        "            df = pd.read_csv(os.path.join(data_main_path, file), sep='\\t')\n",
        "            \n",
        "    with np.errstate(invalid=\"ignore\"):\n",
        "        corr = np.nan_to_num(df.corr(method='spearman'))\n",
        "        mask = np.invert(np.tri(corr.shape[0], k=-1, dtype=bool))\n",
        "        m = ma.masked_where(mask == 1, mask)\n",
        "        return ma.masked_where(m, corr).compressed()\n",
        "\n",
        "def get_corr_matrix(filename):\n",
        "    for file in os.listdir(data_main_path):\n",
        "        if file.startswith(filename):\n",
        "            df = pd.read_csv(os.path.join(data_main_path, file), sep='\\t')\n",
        "    with np.errstate(invalid=\"ignore\"):\n",
        "        corr = np.nan_to_num(df.corr(method='spearman'))\n",
        "        return corr\n",
        "\n",
        "def confusion(g_turth,predictions):\n",
        "    tn, fp, fn, tp = confusion_matrix(g_turth,predictions).ravel()\n",
        "    accuracy = (tp+tn)/(tp+fp+tn+fn)\n",
        "    sensitivity = (tp)/(tp+fn)\n",
        "    specificty = (tn)/(tn+fp)\n",
        "    return accuracy,sensitivity,specificty\n",
        "\n",
        "def get_regs(samplesnames,regnum):\n",
        "    datas = []\n",
        "    for sn in samplesnames:\n",
        "        datas.append(all_corr[sn][0])\n",
        "    datas = np.array(datas)\n",
        "    avg=[]\n",
        "    for ie in range(datas.shape[1]):\n",
        "        avg.append(np.mean(datas[:,ie]))\n",
        "    avg=np.array(avg)\n",
        "    highs=avg.argsort()[-regnum:][::-1]\n",
        "    lows=avg.argsort()[:regnum][::-1]\n",
        "    regions=np.concatenate((highs,lows),axis=0)\n",
        "    return regions\n"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VczoIJ5N1nUF"
      },
      "source": [
        "## Helper functions for computing correlations\n",
        "\n",
        "The correlation matrix is saved as an pkl file, so that it can be called directly during training to saving time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SRhqQYB1nUA"
      },
      "source": [
        "p_ROI = \"cc200\""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_Wf0YIP1nUF"
      },
      "source": [
        "if not os.path.exists('./correlations_file'+p_ROI+'.pkl'):\n",
        "    pbar=pyprind.ProgBar(len(flist))\n",
        "    all_corr = {}\n",
        "    for f in flist:\n",
        "      \n",
        "        lab = get_label(f)\n",
        "        all_corr[f] = (get_corr_data(f), lab)\n",
        "        pbar.update()\n",
        "\n",
        "    print('Corr-computations finished')\n",
        "\n",
        "    pickle.dump(all_corr, open('./correlations_file'+p_ROI+'.pkl', 'wb'))\n",
        "    print('Saving to file finished')\n",
        "\n",
        "else:\n",
        "    all_corr = pickle.load(open('./correlations_file'+p_ROI+'.pkl', 'rb'))"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7i0Te0F1nUG"
      },
      "source": [
        "## Computing eigenvalues and eigenvector\n",
        "\n",
        "The mean of all samples correlation matrix is calculated. Those locations with 1/2 highest score in average correlation matrix is selected and taken as mask, the correlation matrix of each sample is filtered by the mask and flattened to a feature vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zf9Yccm61nUG",
        "outputId": "a1c3621d-b7f0-4871-f761-76f6f0104be3"
      },
      "source": [
        "eig_data = {}\n",
        "pbar = pyprind.ProgBar(len(flist))\n",
        "for f in flist:  \n",
        "      d = get_corr_matrix(f)\n",
        "      eig_vals, eig_vecs = np.linalg.eig(d)\n",
        "\n",
        "      for ev in eig_vecs.T:\n",
        "          np.testing.assert_array_almost_equal(1.0, np.linalg.norm(ev))\n",
        "\n",
        "      sum_eigvals = np.sum(np.abs(eig_vals))\n",
        "      # Make a list of (eigenvalue, eigenvector, norm_eigval) tuples\n",
        "      eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i], np.abs(eig_vals[i])/sum_eigvals)\n",
        "                    for i in range(len(eig_vals))]\n",
        "\n",
        "      # Sort the (eigenvalue, eigenvector) tuples from high to low\n",
        "      eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "      eig_data[f] = {'eigvals':np.array([ep[0] for ep in eig_pairs]),\n",
        "                      'norm-eigvals':np.array([ep[2] for ep in eig_pairs]),\n",
        "                      'eigvecs':[ep[1] for ep in eig_pairs]}\n",
        "      pbar.update()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0% [##############################] 100% | ETA: 00:00:00\n",
            "Total time elapsed: 00:00:06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCCJRDIF1nUH"
      },
      "source": [
        "## Calculating Eros similarity\n",
        "\n",
        "EROS is a distance metric which is applied to measure the similarity between different samples. The data augmentation is perform between samples and their nearest neighbors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_tW48eB1nUH"
      },
      "source": [
        "def norm_weights(sub_flist):\n",
        "    num_dim = len(eig_data[flist[0]]['eigvals'])\n",
        "    norm_weights = np.zeros(shape=num_dim)\n",
        "    for f in sub_flist:\n",
        "        norm_weights += eig_data[f]['norm-eigvals'] \n",
        "    return norm_weights\n",
        "\n",
        "def cal_similarity(d1, d2, weights, lim=None):\n",
        "    res = 0.0\n",
        "    if lim is None:\n",
        "        weights_arr = weights.copy()\n",
        "    else:\n",
        "        weights_arr = weights[:lim].copy()\n",
        "        weights_arr /= np.sum(weights_arr)\n",
        "    for i,w in enumerate(weights_arr):\n",
        "        res += w*np.inner(d1[i], d2[i])\n",
        "    return res"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoNGPxEaYqsE"
      },
      "source": [
        "# Defining Mixup Function(data augmentation)\n",
        "\n",
        "Mixup is an linear interpolation method, the feature vector of a new sample is generated based on a linear formula. The sample size is doubled after augmentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FS9a7DXYvPA"
      },
      "source": [
        "def mixup(x1,x2,y1,y2,alpha,beta):\n",
        "    \"\"\"\n",
        "    get batch data\n",
        "    :param x: training data\n",
        "    :param y: one-hot label\n",
        "    :param step: step\n",
        "    :param batch_size: batch size\n",
        "    :param alpha: hyper-parameter α, default as 0.2\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    if alpha == 0:\n",
        "        return x,y\n",
        "    if alpha > 0:\n",
        "        weight = np.random.beta(alpha, beta, None)\n",
        "        x_weight = weight\n",
        "        y_weight = weight\n",
        "        x = x1 * x_weight + x2 * (1 - x_weight)\n",
        "        y = y1 * y_weight + y2 * (1 - y_weight)\n",
        "        return x, y"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFDuYHAt1nUI"
      },
      "source": [
        "## Defining dataset class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEZR_u301nUI"
      },
      "source": [
        "class CC200Dataset(Dataset):\n",
        "    def __init__(self, pkl_filename=None, data=None, samples_list=None, \n",
        "                 augmentation=False, aug_factor=2, num_neighbs=5,\n",
        "                 eig_data=None, similarity_fn=None, verbose=False,regs=None):\n",
        "        self.regs=regs\n",
        "        if pkl_filename is not None:\n",
        "            if verbose:\n",
        "                print ('Loading ..!', end=' ')\n",
        "            self.data = pickle.load(open(pkl_filename, 'rb'))\n",
        "        elif data is not None:\n",
        "            self.data = data.copy()\n",
        "            \n",
        "        else:\n",
        "            sys.stderr.write('Eigther PKL file or data is needed!')\n",
        "            return \n",
        "\n",
        "        if samples_list is None:\n",
        "            self.flist = [f for f in self.data]\n",
        "        else:\n",
        "            self.flist = [f for f in samples_list]\n",
        "        self.labels = np.array([self.data[f][1] for f in self.flist])\n",
        "        \n",
        "        current_flist = np.array(self.flist.copy())\n",
        "        current_lab0_flist = current_flist[self.labels == 0]\n",
        "        current_lab1_flist = current_flist[self.labels == 1]\n",
        "\n",
        "        \n",
        "        \n",
        "        if augmentation:\n",
        "            self.num_data = aug_factor * len(self.flist)\n",
        "            self.neighbors = {}\n",
        "            pbar = pyprind.ProgBar(len(self.flist))\n",
        "            weights = norm_weights(samples_list)#??\n",
        "            for f in self.flist:\n",
        "                label = self.data[f][1]\n",
        "                candidates = (set(current_lab0_flist) if label == 0 else set(current_lab1_flist))\n",
        "                candidates.remove(f)\n",
        "                eig_f = eig_data[f]['eigvecs']\n",
        "                sim_list = []\n",
        "                for cand in candidates:\n",
        "                    eig_cand = eig_data[cand]['eigvecs']\n",
        "                    sim = similarity_fn(eig_f, eig_cand,weights)\n",
        "                    sim_list.append((sim, cand))\n",
        "                sim_list.sort(key=lambda x: x[0], reverse=True)\n",
        "                self.neighbors[f] = [item[1] for item in sim_list[:num_neighbs]]#list(candidates)#[item[1] for item in sim_list[:num_neighbs]]\n",
        "        \n",
        "        else:\n",
        "            self.num_data = len(self.flist)\n",
        "\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        if index < len(self.flist):\n",
        "            fname = self.flist[index]\n",
        "            data = self.data[fname][0].copy() #get_corr_data(fname, mode=cal_mode)    \n",
        "            data = data[self.regs].copy()\n",
        "            label = (self.labels[index],)\n",
        "            return torch.FloatTensor(data), torch.FloatTensor(label)\n",
        "        else:\n",
        "            f1 = self.flist[index % len(self.flist)]\n",
        "            d1, y1 = self.data[f1][0], self.data[f1][1]\n",
        "            d1=d1[self.regs]\n",
        "            f2=np.random.choice(self.neighbors[f1])\n",
        "            # f2 = np.random.choice(self.flist)\n",
        "            d2, y2 = self.data[f2][0], self.data[f2][1]\n",
        "            d2=d2[self.regs]\n",
        "            # assert y1 == y2\n",
        "            # r = np.random.uniform(low=0, high=1)\n",
        "            # label = (y1,)\n",
        "            # data = r*d1 + (1-r)*d2\n",
        "            data,label=mixup(d1,d2,y1,y2,0.2,0.2)\n",
        "            assert label>=0 and label<=1\n",
        "            if label>=0 and label<0.5:\n",
        "              label=0\n",
        "            elif label>0.5 and label<=1:\n",
        "              label=1\n",
        "            else:\n",
        "              label=np.random.randint(0,2)\n",
        "            label=(label,)\n",
        "            return torch.FloatTensor(data), torch.FloatTensor(label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_data"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puuy9op51nUI"
      },
      "source": [
        "## Definig data loader function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9Odb77E1nUJ"
      },
      "source": [
        "def get_loader(pkl_filename=None, data=None, samples_list=None,\n",
        "               batch_size=64, \n",
        "               num_workers=1, mode='train',\n",
        "               *, augmentation=False, aug_factor=1, num_neighbs=5,\n",
        "                 eig_data=None, similarity_fn=None, verbose=False,regions=None):\n",
        "    \"\"\"Build and return data loader.\"\"\"\n",
        "    if mode == 'train':\n",
        "        shuffle = True\n",
        "    else:\n",
        "        shuffle = False\n",
        "        augmentation=False\n",
        "\n",
        "    dataset = CC200Dataset(pkl_filename=pkl_filename, data=data, samples_list=samples_list,\n",
        "                           augmentation=augmentation, aug_factor=aug_factor, \n",
        "                           eig_data=eig_data, similarity_fn=similarity_fn, verbose=verbose,regs=regions)\n",
        "\n",
        "    data_loader = DataLoader(dataset,\n",
        "                             batch_size=batch_size,\n",
        "                             shuffle=shuffle,\n",
        "                             num_workers=num_workers)\n",
        "  \n",
        "    return data_loader"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acdtc4OVEVL4"
      },
      "source": [
        "# define loss function\n",
        "def smooth_l1_loss(input, target, sigma=1, reduce=True, normalizer=1.0):\n",
        "    beta = 1. / (sigma ** 2)\n",
        "    diff = torch.abs(input - target)\n",
        "    cond = diff < beta\n",
        "    loss = torch.where(cond, 0.5 * diff ** 2 / beta, diff - 0.5 * beta)\n",
        "    if reduce:\n",
        "        return torch.sum(loss) / normalizer\n",
        "    return torch.sum(loss, dim=1) / normalizer"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Perform model training and evaluation(Mixup+VAE+DNN)**"
      ],
      "metadata": {
        "id": "9b5o8EB8kZMP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ec79fcDl1nUK",
        "outputId": "4c0e15d3-a2e3-40dd-f572-9a4a888beaec"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define DNN classifier"
      ],
      "metadata": {
        "id": "wvIzH7AC8vHx"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNMNyu3IG8p-"
      },
      "source": [
        "class DNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(DNN, self).__init__()\n",
        "    \n",
        "    self.fc1 = nn.Linear(9950,4975)\n",
        "    self.relu1 = nn.ReLU()\n",
        "    self.fc2 = nn.Linear(4975,2000)\n",
        "    self.relu2 = nn.ReLU()\n",
        "    self.fc3 = nn.Linear(2000,1)\n",
        "\n",
        "  def forward(self, input_data):\n",
        "    data = Variable(input_data).to(device)\n",
        "    x = self.fc1(data) \n",
        "    x = self.relu1(x) \n",
        "    x = self.fc2(x) \n",
        "    x = self.relu2(x)\n",
        "    pred = self.fc3(x) \n",
        "    \n",
        "    return pred"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaxLs9HT1nUJ"
      },
      "source": [
        "## Define Variational Autoencoder class\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bSbAmi21nUJ",
        "outputId": "40f2471c-d438-4b65-cde8-8bf059b81081"
      },
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, DNN,num_inputs=990, \n",
        "                 num_latent=200, tied=False,\n",
        "                 num_classes=2, use_dropout=True):\n",
        "        super(VAE, self).__init__()\n",
        "        self.dnn = DNN()\n",
        "        self.fc1 = nn.Linear(num_inputs, 400)\n",
        "        self.fc2 = nn.Linear(400, num_latent) \n",
        "        self.fc3 = nn.Linear(400, num_latent) \n",
        "        self.fc4 = nn.Linear(num_latent, 400)\n",
        "        self.fc5 = nn.Linear(400, num_inputs)\n",
        "        if use_dropout:\n",
        "            self.classifier = nn.Sequential (\n",
        "                nn.Dropout(p=0.2),\n",
        "                self.dnn,\n",
        "                \n",
        "            )\n",
        "        else:\n",
        "            self.classifier = nn.Sequential (\n",
        "                self.dnn,\n",
        "            )\n",
        "    # define endcoder\n",
        "    def encode(self, x):\n",
        "        h = F.relu(self.fc1(x))\n",
        "        return self.fc2(h), self.fc3(h)\n",
        "    \n",
        "    # define reparameter function(random sampling)\n",
        "    def reparameterize(self, mu, log_var):\n",
        "        std = torch.exp(log_var/2)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        " \n",
        "    # define decoder\n",
        "    def decode(self, z):\n",
        "        h = F.relu(self.fc4(z))\n",
        "        return F.sigmoid(self.fc5(h))\n",
        "    \n",
        "    # define forward training function\n",
        "    def forward(self, x, eval_classifier = False):\n",
        "        mu, log_var = self.encode(x)\n",
        "        z = self.reparameterize(mu, log_var)\n",
        "        x_reconst = self.decode(z)\n",
        "        if eval_classifier:\n",
        "            x_logit = self.classifier(x)\n",
        "        else:\n",
        "            x_logit = None\n",
        "        return x_reconst, mu, log_var, x_logit\n",
        "mtae = VAE(DNN)\n",
        "mtae"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VAE(\n",
              "  (dnn): DNN(\n",
              "    (fc1): Linear(in_features=9950, out_features=4975, bias=True)\n",
              "    (relu1): ReLU()\n",
              "    (fc2): Linear(in_features=4975, out_features=2000, bias=True)\n",
              "    (relu2): ReLU()\n",
              "    (fc3): Linear(in_features=2000, out_features=1, bias=True)\n",
              "  )\n",
              "  (fc1): Linear(in_features=990, out_features=400, bias=True)\n",
              "  (fc2): Linear(in_features=400, out_features=200, bias=True)\n",
              "  (fc3): Linear(in_features=400, out_features=200, bias=True)\n",
              "  (fc4): Linear(in_features=200, out_features=400, bias=True)\n",
              "  (fc5): Linear(in_features=400, out_features=990, bias=True)\n",
              "  (classifier): Sequential(\n",
              "    (0): Dropout(p=0.2, inplace=False)\n",
              "    (1): DNN(\n",
              "      (fc1): Linear(in_features=9950, out_features=4975, bias=True)\n",
              "      (relu1): ReLU()\n",
              "      (fc2): Linear(in_features=4975, out_features=2000, bias=True)\n",
              "      (relu2): ReLU()\n",
              "      (fc3): Linear(in_features=2000, out_features=1, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nr84NsRE1nUJ"
      },
      "source": [
        "## Define training and testing functions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlW6Dfbp1nUJ"
      },
      "source": [
        "def train(model, epoch, train_loader, p_bernoulli=None, mode='both', lam_factor=1.0):\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "    for i,(batch_x,batch_y) in enumerate(train_loader):\n",
        "        if len(batch_x) != batch_size:\n",
        "            continue\n",
        "        if p_bernoulli is not None:\n",
        "            if i == 0:\n",
        "                p_tensor = torch.ones_like(batch_x).to(device)*p_bernoulli\n",
        "            rand_bernoulli = torch.bernoulli(p_tensor).to(device)\n",
        "\n",
        "        data, target = batch_x.to(device), batch_y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if mode in ['both', 'ae']:\n",
        "            if p_bernoulli is not None:\n",
        "                rec_noisy,mu,log_var,_ = model(data*rand_bernoulli, False)\n",
        "                kl_div = - 0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "                loss_ae = (smooth_l1_loss(rec_noisy, data)+kl_div) / len(batch_x)\n",
        "            else:\n",
        "                rec,mu,log_var,_ = model(data, False)\n",
        "                kl_div = - 0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "                loss_ae = (smooth_l1_loss(rec, data)+kl_div) / len(batch_x)\n",
        "        \n",
        "        if mode in ['both', 'clf']:\n",
        "            rec_clean,mu2,var2, logits = model(data, True)\n",
        "            loss_clf = criterion_clf(logits, target)\n",
        "\n",
        "        if mode == 'both':\n",
        "            loss_total = loss_ae + lam_factor*loss_clf\n",
        "            train_losses.append([loss_ae.detach().cpu().numpy(), \n",
        "                                 loss_clf.detach().cpu().numpy()])\n",
        "        elif mode == 'ae':\n",
        "            loss_total = loss_ae\n",
        "            train_losses.append([loss_ae.detach().cpu().numpy(), \n",
        "                                 0.0])\n",
        "        elif mode == 'clf':\n",
        "            loss_total = loss_clf\n",
        "            train_losses.append([0.0, \n",
        "                                 loss_clf.detach().cpu().numpy()])\n",
        "\n",
        "        loss_total.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return train_losses       \n",
        "\n",
        "def test(model, criterion, test_loader, \n",
        "         eval_classifier=False, num_batch=None):\n",
        "    test_loss, n_test, correct = 0.0, 0, 0\n",
        "    all_predss=[]\n",
        "    if eval_classifier:\n",
        "        y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for i,(batch_x,batch_y) in enumerate(test_loader, 1):\n",
        "            if num_batch is not None:\n",
        "                if i >= num_batch:\n",
        "                    continue\n",
        "            data = batch_x.to(device)\n",
        "            rec,mu,log_var, logits = model(data, eval_classifier)\n",
        "            kl_div = - 0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "            newloss = smooth_l1_loss(rec, data)+kl_div\n",
        "            test_loss += newloss.detach().cpu().numpy() \n",
        "            n_test += len(batch_x)\n",
        "            if eval_classifier:\n",
        "                proba = torch.sigmoid(logits).detach().cpu().numpy()\n",
        "                preds = np.ones_like(proba, dtype=np.int32)\n",
        "                preds[proba < 0.5] = 0\n",
        "                all_predss.extend(preds)\n",
        "                y_arr = np.array(batch_y, dtype=np.int32)\n",
        "\n",
        "                correct += np.sum(preds == y_arr)\n",
        "                y_true.extend(y_arr.tolist())\n",
        "                y_pred.extend(proba.tolist())\n",
        "        mlp_acc,mlp_sens,mlp_spef = confusion(y_true,all_predss)\n",
        "\n",
        "    return  mlp_acc,mlp_sens,mlp_spef#,correct/n_test"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the model on whole samples"
      ],
      "metadata": {
        "id": "E7kffSiK1ArF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p_Method = 'Mixup+VAE+DNN'\n",
        "p_mode = \"whole\"\n",
        "p_fold = 10\n",
        "p_augmentation = True"
      ],
      "metadata": {
        "id": "m_xFN24RAToR"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNouJOhe1nUK",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5a934a7-6a38-48fc-e158-c3ebeb65ade3"
      },
      "source": [
        "if p_Method == \"Mixup+VAE+DNN\" and p_mode == \"whole\":\n",
        "    \n",
        "    num_corr = len(all_corr[flist[0]][0])\n",
        "    print(\"num_corr:  \",num_corr)\n",
        "    \n",
        "    start =time.time()\n",
        "    batch_size = 8\n",
        "    learning_rate_ae, learning_rate_clf = 0.0001, 0.0001\n",
        "    num_epochs = 25\n",
        "\n",
        "    p_bernoulli = None\n",
        "    augmentation = p_augmentation\n",
        "    use_dropout = False\n",
        "\n",
        "    aug_factor = 2\n",
        "    num_neighbs = 5\n",
        "    lim4sim = 2\n",
        "    n_lat = int(num_corr/4)\n",
        "    print(n_lat)\n",
        "    start= time.time()\n",
        "\n",
        "    print('p_bernoulli: ', p_bernoulli)\n",
        "    print('augmentaiton: ', augmentation, 'aug_factor: ', aug_factor, \n",
        "          'num_neighbs: ', num_neighbs, 'lim4sim: ', lim4sim)\n",
        "    print('use_dropout: ', use_dropout, '\\n')\n",
        "\n",
        "\n",
        "    sim_function = functools.partial(cal_similarity, lim=lim4sim)\n",
        "    crossval_res_kol=[]\n",
        "    y_arr = np.array([get_label(f) for f in flist])\n",
        "    flist = np.array(flist)\n",
        "    kk=0 \n",
        "    for rp in range(1):\n",
        "        kf = StratifiedKFold(n_splits=p_fold, random_state=1, shuffle=True)\n",
        "        np.random.shuffle(flist)\n",
        "        y_arr = np.array([get_label(f) for f in flist])\n",
        "        for kk,(train_index, test_index) in enumerate(kf.split(flist, y_arr)):\n",
        "            train_samples, test_samples = flist[train_index], flist[test_index]\n",
        "\n",
        "\n",
        "            verbose = (True if (kk == 0) else False)\n",
        "\n",
        "            regions_inds = get_regs(train_samples,int(num_corr/4))\n",
        "\n",
        "            num_inpp = len(regions_inds)\n",
        "            n_lat = int(num_inpp/1)\n",
        "            train_loader=get_loader(data=all_corr, samples_list=train_samples, \n",
        "                                    batch_size=batch_size, mode='train',\n",
        "                                    augmentation=augmentation, aug_factor=aug_factor, \n",
        "                                    num_neighbs=num_neighbs, eig_data=eig_data, similarity_fn=sim_function, \n",
        "                                    verbose=verbose,regions=regions_inds)\n",
        "\n",
        "            test_loader=get_loader(data=all_corr, samples_list=test_samples, \n",
        "                                   batch_size=batch_size, mode='test', augmentation=False, \n",
        "                                   verbose=verbose,regions=regions_inds)\n",
        "\n",
        "            model = VAE(DNN, tied=False, num_inputs=num_inpp,num_latent=n_lat, use_dropout=use_dropout)\n",
        "            model.to(device)\n",
        "            \n",
        "            criterion_clf = nn.BCEWithLogitsLoss()\n",
        "            optimizer = optim.Adam(model.parameters(), lr=learning_rate_ae)\n",
        "            #optimizer = optim.SGD([{'params': model.encode.parameters(), 'lr': learning_rate_ae},\n",
        "                                   #{'params': model.classifier.parameters(), 'lr': learning_rate_clf}],\n",
        "                                  #momentum=0.9)\n",
        "\n",
        "            for epoch in range(10, num_epochs+1):\n",
        "                if epoch <= 20:\n",
        "                    train_losses = train(model, epoch, train_loader, p_bernoulli, mode='both')\n",
        "                else:\n",
        "                    train_losses = train(model, epoch, train_loader, p_bernoulli, mode='clf')\n",
        "\n",
        "      \n",
        "            res_mlp = test(model, smooth_l1_loss, test_loader, eval_classifier=True)\n",
        "            print(test(model, smooth_l1_loss, test_loader, eval_classifier=True))\n",
        "            crossval_res_kol.append(res_mlp)\n",
        "        print(\"averages:\")\n",
        "        print(np.mean(np.array(crossval_res_kol),axis = 0))\n",
        "        finish= time.time()\n",
        "\n",
        "        print(finish-start)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_corr:   19900\n",
            "4975\n",
            "p_bernoulli:  None\n",
            "augmentaiton:  True aug_factor:  2 num_neighbs:  5 lim4sim:  2\n",
            "use_dropout:  False \n",
            "\n",
            "(0.6923076923076923, 0.5882352941176471, 0.7924528301886793)\n",
            "(0.7115384615384616, 0.6666666666666666, 0.7547169811320755)\n",
            "(0.6346153846153846, 0.6274509803921569, 0.6415094339622641)\n",
            "(0.6826923076923077, 0.5882352941176471, 0.7735849056603774)\n",
            "(0.7403846153846154, 0.7647058823529411, 0.7169811320754716)\n",
            "(0.7087378640776699, 0.66, 0.7547169811320755)\n",
            "(0.6990291262135923, 0.7, 0.6981132075471698)\n",
            "(0.6796116504854369, 0.58, 0.7735849056603774)\n",
            "(0.6504854368932039, 0.68, 0.6226415094339622)\n",
            "(0.6407766990291263, 0.68, 0.6037735849056604)\n",
            "averages:\n",
            "[0.68401792 0.65352941 0.71320755]\n",
            "1566.5823955535889\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the model on each site"
      ],
      "metadata": {
        "id": "3ucu1cvX0f7R"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9im9ITdaQrUI"
      },
      "source": [
        "# set parameter for the model\n",
        "p_Method = 'Mixup+VAE+DNN'\n",
        "p_mode=\"percenter\"\n",
        "p_center=\"Stanford\"\n",
        "p_fold = 5\n",
        "p_augmentation = False"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPVr7i9CO1Lw",
        "outputId": "6dacd4f9-2684-4da7-8088-ec3febd8b44c"
      },
      "source": [
        "if p_Method == \"Mixup+VAE+DNN\" and p_mode == \"percenter\":\n",
        "    num_corr = len(all_corr[flist[0]][0])\n",
        "\n",
        "    flist = os.listdir(data_main_path)\n",
        "\n",
        "    for f in range(len(flist)):\n",
        "        flist[f] = get_key(flist[f])\n",
        "    \n",
        "    centers_dict = {}\n",
        "    for f in flist:\n",
        "        key = f.split('_')[0]\n",
        "\n",
        "        if key not in centers_dict:\n",
        "            centers_dict[key] = []\n",
        "        centers_dict[key].append(f)\n",
        "\n",
        "    \n",
        "\n",
        "    flist = np.array(centers_dict[p_center])\n",
        "    \n",
        "    start =time.time()\n",
        "    #flist = np.array(sorted(os.listdir(data_main_path)))\n",
        "    batch_size = 8\n",
        "    learning_rate_ae, learning_rate_clf = 0.0001, 0.0001\n",
        "    num_epochs = 25\n",
        "\n",
        "    p_bernoulli = None\n",
        "    augmentation = p_augmentation\n",
        "    use_dropout = False\n",
        "\n",
        "    aug_factor = 2\n",
        "    num_neighbs = 5\n",
        "    lim4sim = 2\n",
        "    n_lat = int(num_corr/4)\n",
        "\n",
        "\n",
        "    print('p_bernoulli: ', p_bernoulli)\n",
        "    print('augmentaiton: ', augmentation, 'aug_factor: ', aug_factor, \n",
        "          'num_neighbs: ', num_neighbs, 'lim4sim: ', lim4sim)\n",
        "    print('use_dropout: ', use_dropout, '\\n')\n",
        "\n",
        "\n",
        "    sim_function = functools.partial(cal_similarity, lim=lim4sim)\n",
        "    all_rp_res=[]\n",
        "    y_arr = np.array([get_label(f) for f in flist])\n",
        "\n",
        "    kk=0 \n",
        "    crossval_res_kol_kol=[]\n",
        "    for rp in range(10):\n",
        "        print(\"========================\")\n",
        "        crossval_res_kol = []\n",
        "        start= time.time()\n",
        "        kf = StratifiedKFold(n_splits=p_fold)\n",
        "        #np.random.shuffle(flist)\n",
        "        y_arr = np.array([get_label(f) for f in flist])\n",
        "        for kk,(train_index, test_index) in enumerate(kf.split(flist, y_arr)):\n",
        "        \n",
        "            train_samples, test_samples = flist[train_index], flist[test_index]\n",
        "\n",
        "            verbose = (True if (kk == 0) else False)\n",
        "\n",
        "            regions_inds = get_regs(train_samples,int(num_corr/4))\n",
        "            num_inpp = len(regions_inds)\n",
        "            n_lat = int(num_inpp/1)\n",
        "            num_inpp = len(regions_inds)\n",
        "            train_loader=get_loader(data=all_corr, samples_list=train_samples, \n",
        "                                    batch_size=batch_size, mode='train',\n",
        "                                    augmentation=augmentation, aug_factor=aug_factor, \n",
        "                                    num_neighbs=num_neighbs, eig_data=eig_data, similarity_fn=sim_function, \n",
        "                                    verbose=verbose,regions=regions_inds)\n",
        "\n",
        "            test_loader=get_loader(data=all_corr, samples_list=test_samples, \n",
        "                                   batch_size=batch_size, mode='test', augmentation=False, \n",
        "                                   verbose=verbose,regions=regions_inds)\n",
        "\n",
        "            model = VAE(DNN, tied=False, num_inputs=num_inpp,num_latent=n_lat, use_dropout=use_dropout)\n",
        "            model.to(device)\n",
        "            criterion_clf = nn.BCEWithLogitsLoss()\n",
        "            optimizer = optim.Adam(model.parameters(), lr=learning_rate_ae)\n",
        "\n",
        "            for epoch in range(10, num_epochs+1):\n",
        "                if epoch <= 20:\n",
        "                    train_losses = train(model, epoch, train_loader, p_bernoulli, mode='both')\n",
        "                else:\n",
        "                    train_losses = train(model, epoch, train_loader, p_bernoulli, mode='clf')\n",
        "\n",
        "\n",
        "\n",
        "            res_mlp = test(model, smooth_l1_loss, test_loader, eval_classifier=True)\n",
        "            #print(\"fold\",kk+1,\":\",test(model, criterion_ae, test_loader, eval_classifier=True))\n",
        "            crossval_res_kol.append(res_mlp)\n",
        "        print(\"Result of repeat \",rp,\":\")\n",
        "        print(np.mean(np.array(crossval_res_kol),axis = 0))\n",
        "        all_rp_res.append(np.mean(np.array(crossval_res_kol),axis = 0))\n",
        "        finish= time.time()\n",
        "\n",
        "        print(\"Running time:\",finish-start)\n",
        "    print(\"Avergae result of 10 repeats: \",np.mean(np.array(all_rp_res),axis = 0))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p_bernoulli:  None\n",
            "augmentaiton:  False aug_factor:  2 num_neighbs:  5 lim4sim:  2\n",
            "use_dropout:  False \n",
            "\n",
            "========================\n",
            "Result of repeat  0 :\n",
            "[0.69285714 0.58333333 0.8       ]\n",
            "Running time: 19.491913080215454\n",
            "========================\n",
            "Result of repeat  1 :\n",
            "[0.66785714 0.43333333 0.9       ]\n",
            "Running time: 19.54474949836731\n",
            "========================\n",
            "Result of repeat  2 :\n",
            "[0.64642857 0.55       0.75      ]\n",
            "Running time: 21.978393077850342\n",
            "========================\n",
            "Result of repeat  3 :\n",
            "[0.67142857 0.4        0.95      ]\n",
            "Running time: 19.619850635528564\n",
            "========================\n",
            "Result of repeat  4 :\n",
            "[0.66785714 0.43333333 0.9       ]\n",
            "Running time: 19.623234510421753\n",
            "========================\n",
            "Result of repeat  5 :\n",
            "[0.74642857 0.6        0.9       ]\n",
            "Running time: 20.231318712234497\n",
            "========================\n",
            "Result of repeat  6 :\n",
            "[0.58928571 0.46666667 0.7       ]\n",
            "Running time: 19.655035495758057\n",
            "========================\n",
            "Result of repeat  7 :\n",
            "[0.69642857 0.5        0.9       ]\n",
            "Running time: 19.766722440719604\n",
            "========================\n",
            "Result of repeat  8 :\n",
            "[0.64642857 0.5        0.8       ]\n",
            "Running time: 19.69882082939148\n",
            "========================\n",
            "Result of repeat  9 :\n",
            "[0.64285714 0.53333333 0.75      ]\n",
            "Running time: 19.550172567367554\n",
            "Avergae result of 10 repeats:  [0.66678571 0.5        0.835     ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Perform model training and evaluation(Mixup+SAE+DNN)**"
      ],
      "metadata": {
        "id": "QPPGZETqknQw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define DNN classifer for SAE"
      ],
      "metadata": {
        "id": "AoSfvHMq2uWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DNN(nn.Module):\n",
        "  def __init__(self):  \n",
        "    super(DNN, self).__init__()\n",
        "    \n",
        "    self.fc1 = nn.Linear(4975,2000)\n",
        "    self.drop1 = nn.Dropout(p=0.5)\n",
        "    self.relu1 = nn.ReLU()\n",
        "    self.fc2 = nn.Linear(2000,2)\n",
        "    self.norm2 = nn.BatchNorm1d(2)\n",
        "    self.relu2 = nn.ReLU()\n",
        "    self.fc3 = nn.Linear(2,1)\n",
        "\n",
        "  def forward(self, input_data):\n",
        "    data = Variable(input_data).to(device)\n",
        "    x = self.fc1(data)\n",
        "    x = self.drop1(x)\n",
        "    x = self.relu1(x)\n",
        "    x = self.fc2(x)\n",
        "    x = self.norm2(x)\n",
        "    x = self.relu2(x)\n",
        "    pred = self.fc3(x)\n",
        "\n",
        "    return pred"
      ],
      "metadata": {
        "id": "mLmkeOlhmC8x"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Sparse Autoencoder class"
      ],
      "metadata": {
        "id": "Jmb7p2QwmiWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AutoEncoderDNN(nn.Module):\n",
        "    def __init__(self, DNN, num_inputs=990, \n",
        "                 num_latent=200, tied=True,\n",
        "                 num_classes=2, use_dropout=False):\n",
        "        super(AutoEncoderDNN, self).__init__()\n",
        "        self.tied = tied\n",
        "        self.num_latent = num_latent\n",
        "        self.dnn = DNN()\n",
        "        \n",
        "        self.fc_encoder = nn.Linear(num_inputs, num_latent)\n",
        "    \n",
        "        if not tied:\n",
        "            self.fc_decoder = nn.Linear(num_latent, num_inputs)\n",
        "         \n",
        "        self.fc_encoder = nn.Linear(num_inputs, num_latent)\n",
        "        \n",
        "        if use_dropout:\n",
        "            self.classifier = nn.Sequential (\n",
        "                nn.Dropout(p=0.5),\n",
        "                self.dnn,\n",
        "                \n",
        "            )\n",
        "        else:\n",
        "            self.classifier = nn.Sequential (\n",
        "                self.dnn,\n",
        "            )\n",
        "            \n",
        "         \n",
        "    def forward(self, x, eval_classifier=False):\n",
        "        x = self.fc_encoder(x)\n",
        "        x = torch.tanh(x)\n",
        "        if eval_classifier: \n",
        "            x_logit = self.classifier(x)\n",
        "        else:\n",
        "            x_logit = None\n",
        "        \n",
        "        if self.tied: \n",
        "            x = F.linear(x, self.fc_encoder.weight.t())\n",
        "        else:\n",
        "            x = self.fc_decoder(x)\n",
        "            \n",
        "        return x, x_logit \n",
        "\n",
        "aednn = AutoEncoderDNN(DNN)\n",
        "\n",
        "aednn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fp2ZzSbKmG-T",
        "outputId": "cc405301-f7b2-4cd0-dbee-f67e04913495"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AutoEncoderDNN(\n",
              "  (dnn): DNN(\n",
              "    (fc1): Linear(in_features=4975, out_features=2000, bias=True)\n",
              "    (drop1): Dropout(p=0.5, inplace=False)\n",
              "    (relu1): ReLU()\n",
              "    (fc2): Linear(in_features=2000, out_features=2, bias=True)\n",
              "    (norm2): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu2): ReLU()\n",
              "    (fc3): Linear(in_features=2, out_features=1, bias=True)\n",
              "  )\n",
              "  (fc_encoder): Linear(in_features=990, out_features=200, bias=True)\n",
              "  (classifier): Sequential(\n",
              "    (0): DNN(\n",
              "      (fc1): Linear(in_features=4975, out_features=2000, bias=True)\n",
              "      (drop1): Dropout(p=0.5, inplace=False)\n",
              "      (relu1): ReLU()\n",
              "      (fc2): Linear(in_features=2000, out_features=2, bias=True)\n",
              "      (norm2): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu2): ReLU()\n",
              "      (fc3): Linear(in_features=2, out_features=1, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define training and testing function"
      ],
      "metadata": {
        "id": "A44aUyrP24Wp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, epoch, train_loader, p_bernoulli=None, mode='both', lam_factor=1.0):\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "    for i,(batch_x,batch_y) in enumerate(train_loader):\n",
        "        if len(batch_x) != batch_size:\n",
        "            continue\n",
        "        if p_bernoulli is not None:\n",
        "            if i == 0:\n",
        "                p_tensor = torch.ones_like(batch_x).to(device)*p_bernoulli\n",
        "            rand_bernoulli = torch.bernoulli(p_tensor).to(device)\n",
        "\n",
        "        data, target = batch_x.to(device), batch_y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if mode in ['both', 'ae']:\n",
        "            if p_bernoulli is not None:\n",
        "                rec_noisy, _ = model(data*rand_bernoulli, False)\n",
        "                loss_ae = smooth_l1_loss(rec_noisy, data) / len(batch_x)\n",
        "            else:\n",
        "                rec, _ = model(data, False)\n",
        "                loss_ae = smooth_l1_loss(rec, data) / len(batch_x)\n",
        "\n",
        "        if mode in ['both', 'clf']:\n",
        "            rec_clean, logits = model(data, True)\n",
        "            loss_clf = criterion_clf(logits, target)\n",
        "\n",
        "        if mode == 'both':\n",
        "            loss_total = loss_ae + lam_factor*loss_clf\n",
        "            train_losses.append([loss_ae.detach().cpu().numpy(), \n",
        "                                 loss_clf.detach().cpu().numpy()])\n",
        "        elif mode == 'ae':\n",
        "            loss_total = loss_ae\n",
        "            train_losses.append([loss_ae.detach().cpu().numpy(), \n",
        "                                 0.0])\n",
        "        elif mode == 'clf':\n",
        "            loss_total = loss_clf\n",
        "            train_losses.append([0.0, \n",
        "                                 loss_clf.detach().cpu().numpy()])\n",
        "\n",
        "        loss_total.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return train_losses       \n",
        "\n",
        "def test(model, criterion, test_loader, eval_classifier=False, num_batch=None):\n",
        "    test_loss, n_test, correct = 0.0, 0, 0\n",
        "    all_predss=[]\n",
        "    if eval_classifier:\n",
        "        y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for i,(batch_x,batch_y) in enumerate(test_loader, 1):\n",
        "            if num_batch is not None:\n",
        "                if i >= num_batch:\n",
        "                    continue\n",
        "            data = batch_x.to(device)\n",
        "            rec, logits = model(data, eval_classifier) # model(data,true)\n",
        "            \n",
        "            test_loss += criterion(rec, data).detach().cpu().numpy() \n",
        "            n_test += len(batch_x)\n",
        "            if eval_classifier:\n",
        "                proba = torch.sigmoid(logits).detach().cpu().numpy()\n",
        "                preds = np.ones_like(proba, dtype=np.int32)\n",
        "                preds[proba < 0.5] = 0\n",
        "                all_predss.extend(preds)\n",
        "                y_arr = np.array(batch_y, dtype=np.int32)\n",
        "\n",
        "                correct += np.sum(preds == y_arr)\n",
        "                y_true.extend(y_arr.tolist())\n",
        "                y_pred.extend(proba.tolist())\n",
        "        mlp_acc,mlp_sens,mlp_spef = confusion(y_true,all_predss)\n",
        "\n",
        "    return  mlp_acc,mlp_sens,mlp_spef#,correct/n_test"
      ],
      "metadata": {
        "id": "7EYWW8DSu6mz"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the model on whole samples"
      ],
      "metadata": {
        "id": "X32PtKkxm_EO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p_Method = 'Mixup+SAE+DNN'\n",
        "p_mode = \"whole\"\n",
        "p_fold = 10\n",
        "p_augmentation = True"
      ],
      "metadata": {
        "id": "vCrPNphKnBXi"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if p_Method == \"Mixup+SAE+DNN\" and p_mode == \"whole\":\n",
        "    \n",
        "    num_corr = len(all_corr[flist[0]][0])\n",
        "    print(\"num_corr:  \",num_corr)\n",
        "    \n",
        "    start =time.time()\n",
        "    batch_size = 8\n",
        "    learning_rate_ae, learning_rate_clf = 0.0001, 0.0001\n",
        "    num_epochs = 25\n",
        "\n",
        "    p_bernoulli = None\n",
        "    augmentation = p_augmentation\n",
        "    use_dropout = False\n",
        "\n",
        "    aug_factor = 2\n",
        "    num_neighbs = 5\n",
        "    lim4sim = 2\n",
        "    n_lat = int(num_corr/4)\n",
        "    print(n_lat)\n",
        "    start= time.time()\n",
        "\n",
        "    print('p_bernoulli: ', p_bernoulli)\n",
        "    print('augmentaiton: ', augmentation, 'aug_factor: ', aug_factor, \n",
        "          'num_neighbs: ', num_neighbs, 'lim4sim: ', lim4sim)\n",
        "    print('use_dropout: ', use_dropout, '\\n')\n",
        "\n",
        "\n",
        "    sim_function = functools.partial(cal_similarity, lim=lim4sim)\n",
        "    crossval_res_kol=[]\n",
        "    y_arr = np.array([get_label(f) for f in flist])\n",
        "    flist = np.array(flist)\n",
        "    kk=0 \n",
        "    for rp in range(1):\n",
        "        kf = StratifiedKFold(n_splits=p_fold, random_state=1, shuffle=True)\n",
        "        np.random.shuffle(flist)\n",
        "        y_arr = np.array([get_label(f) for f in flist])\n",
        "        for kk,(train_index, test_index) in enumerate(kf.split(flist, y_arr)):\n",
        "            train_samples, test_samples = flist[train_index], flist[test_index]\n",
        "\n",
        "\n",
        "            verbose = (True if (kk == 0) else False)\n",
        "\n",
        "            regions_inds = get_regs(train_samples,int(num_corr/4))\n",
        "\n",
        "            num_inpp = len(regions_inds)\n",
        "            n_lat = int(num_inpp/2)\n",
        "            train_loader=get_loader(data=all_corr, samples_list=train_samples, \n",
        "                                    batch_size=batch_size, mode='train',\n",
        "                                    augmentation=augmentation, aug_factor=aug_factor, \n",
        "                                    num_neighbs=num_neighbs, eig_data=eig_data, similarity_fn=sim_function, \n",
        "                                    verbose=verbose,regions=regions_inds)\n",
        "\n",
        "            test_loader=get_loader(data=all_corr, samples_list=test_samples, \n",
        "                                   batch_size=batch_size, mode='test', augmentation=False, \n",
        "                                   verbose=verbose,regions=regions_inds)\n",
        "\n",
        "            model = AutoEncoderDNN(DNN,tied=True, num_inputs=num_inpp, num_latent=n_lat, use_dropout=use_dropout)\n",
        "            model.to(device)\n",
        "            \n",
        "            criterion_clf = nn.BCEWithLogitsLoss()\n",
        "            criterion_clf.to(device)\n",
        "            \n",
        "            optimizer = optim.SGD([{'params': model.fc_encoder.parameters(), 'lr': learning_rate_ae},\n",
        "                                   {'params': model.classifier.parameters(), 'lr': learning_rate_clf}],\n",
        "                                  momentum=0.9)\n",
        "            '''\n",
        "            optimizer = optim.Adam([{'params': model.fc_encoder.parameters(), 'lr': learning_rate_ae},\n",
        "                                   {'params': model.classifier.parameters(), 'lr': learning_rate_clf}],\n",
        "                                  momentum=0.9)\n",
        "            '''\n",
        "            for epoch in range(10, num_epochs+1):\n",
        "                if epoch <= 20:\n",
        "                    train_losses = train(model, epoch, train_loader, p_bernoulli, mode='both')\n",
        "                else:\n",
        "                    train_losses = train(model, epoch, train_loader, p_bernoulli, mode='clf')\n",
        "\n",
        "\n",
        "            res_mlp = test(model, smooth_l1_loss, test_loader, eval_classifier=True)\n",
        "            print(test(model, smooth_l1_loss, test_loader, eval_classifier=True))\n",
        "            crossval_res_kol.append(res_mlp)\n",
        "        print(\"averages:\")\n",
        "        print(np.mean(np.array(crossval_res_kol),axis = 0))\n",
        "        finish= time.time()\n",
        "\n",
        "        print(finish-start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmRSJdGWnDWX",
        "outputId": "2def02b7-0b14-45da-df1a-ba24538e5afe"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_corr:   19900\n",
            "4975\n",
            "p_bernoulli:  None\n",
            "augmentaiton:  True aug_factor:  2 num_neighbs:  5 lim4sim:  2\n",
            "use_dropout:  False \n",
            "\n",
            "(0.5, 1.0, 0.0)\n",
            "(0.5, 0.0, 1.0)\n",
            "(0.75, 1.0, 0.5)\n",
            "(0.5, 1.0, 0.0)\n",
            "(0.5, 0.0, 1.0)\n",
            "(0.5, 0.0, 1.0)\n",
            "(0.5, 1.0, 0.0)\n",
            "(0.5, 0.0, 1.0)\n",
            "(0.5, 1.0, 0.0)\n",
            "(0.3333333333333333, 1.0, 0.0)\n",
            "averages:\n",
            "[0.50833333 0.6        0.45      ]\n",
            "62.77608871459961\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the model on each site"
      ],
      "metadata": {
        "id": "gBZ8hBklnGeX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p_Method=\"Mixup+SAE+DNN\"\n",
        "p_mode=\"percenter\"\n",
        "p_center=\"Stanford\"\n",
        "p_fold=5\n",
        "p_augmentation = False"
      ],
      "metadata": {
        "id": "Y8FYxDvSnJAn"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if p_Method == \"Mixup+SAE+DNN\" and p_mode == \"percenter\":\n",
        "    num_corr = len(all_corr[flist[0]][0])\n",
        "\n",
        "    flist = os.listdir(data_main_path)\n",
        "\n",
        "    for f in range(len(flist)):\n",
        "        flist[f] = get_key(flist[f])\n",
        "    \n",
        "    centers_dict = {}\n",
        "    for f in flist:\n",
        "        key = f.split('_')[0]\n",
        "\n",
        "        if key not in centers_dict:\n",
        "            centers_dict[key] = []\n",
        "        centers_dict[key].append(f)\n",
        "\n",
        "    \n",
        "\n",
        "    flist = np.array(centers_dict[p_center])\n",
        "    \n",
        "    start =time.time()\n",
        "    #flist = np.array(sorted(os.listdir(data_main_path)))\n",
        "    batch_size = 8\n",
        "    learning_rate_ae, learning_rate_clf = 0.0001, 0.0001\n",
        "    num_epochs = 25\n",
        "\n",
        "    p_bernoulli = None\n",
        "    augmentation = p_augmentation\n",
        "    use_dropout = False\n",
        "\n",
        "    aug_factor = 2\n",
        "    num_neighbs = 5\n",
        "    lim4sim = 2\n",
        "    n_lat = int(num_corr/4)\n",
        "\n",
        "\n",
        "    print('p_bernoulli: ', p_bernoulli)\n",
        "    print('augmentaiton: ', augmentation, 'aug_factor: ', aug_factor, \n",
        "          'num_neighbs: ', num_neighbs, 'lim4sim: ', lim4sim)\n",
        "    print('use_dropout: ', use_dropout, '\\n')\n",
        "\n",
        "\n",
        "    sim_function = functools.partial(cal_similarity, lim=lim4sim)\n",
        "    all_rp_res=[]\n",
        "    y_arr = np.array([get_label(f) for f in flist])\n",
        "\n",
        "    kk=0 \n",
        "    crossval_res_kol_kol=[]\n",
        "    for rp in range(10):\n",
        "        print(\"========================\")\n",
        "        crossval_res_kol = []\n",
        "        start= time.time()\n",
        "        kf = StratifiedKFold(n_splits=p_fold)\n",
        "        #np.random.shuffle(flist)\n",
        "        y_arr = np.array([get_label(f) for f in flist])\n",
        "        for kk,(train_index, test_index) in enumerate(kf.split(flist, y_arr)):\n",
        "        \n",
        "            train_samples, test_samples = flist[train_index], flist[test_index]\n",
        "\n",
        "            verbose = (True if (kk == 0) else False)\n",
        "\n",
        "            regions_inds = get_regs(train_samples,int(num_corr/4))\n",
        "            num_inpp = len(regions_inds)\n",
        "            n_lat = int(num_inpp/2)\n",
        "            num_inpp = len(regions_inds)\n",
        "            train_loader=get_loader(data=all_corr, samples_list=train_samples, \n",
        "                                    batch_size=batch_size, mode='train',\n",
        "                                    augmentation=augmentation, aug_factor=aug_factor, \n",
        "                                    num_neighbs=num_neighbs, eig_data=eig_data, similarity_fn=sim_function, \n",
        "                                    verbose=verbose,regions=regions_inds)\n",
        "\n",
        "            test_loader=get_loader(data=all_corr, samples_list=test_samples, \n",
        "                                   batch_size=batch_size, mode='test', augmentation=False, \n",
        "                                   verbose=verbose,regions=regions_inds)\n",
        "\n",
        "            model = AutoEncoderDNN(DNN,tied=True, num_inputs=num_inpp, num_latent=n_lat, use_dropout=use_dropout)\n",
        "            model.to(device)\n",
        "\n",
        "            criterion_clf = nn.BCEWithLogitsLoss()\n",
        "            criterion_clf.to(device)\n",
        "            optimizer = optim.SGD([{'params': model.fc_encoder.parameters(), 'lr': learning_rate_ae},\n",
        "                                   {'params': model.classifier.parameters(), 'lr': learning_rate_clf}],\n",
        "                                   momentum=0.9)\n",
        "\n",
        "            for epoch in range(10, num_epochs+1):\n",
        "                if epoch <= 20:\n",
        "                    train_losses = train(model, epoch, train_loader, p_bernoulli, mode='both')\n",
        "                else:\n",
        "                    train_losses = train(model, epoch, train_loader, p_bernoulli, mode='clf')\n",
        "\n",
        "\n",
        "\n",
        "            res_mlp = test(model, smooth_l1_loss, test_loader, eval_classifier=True)\n",
        "            crossval_res_kol.append(res_mlp)\n",
        "        print(\"Result of repeat \",rp,\":\")\n",
        "        print(np.mean(np.array(crossval_res_kol),axis = 0))\n",
        "        all_rp_res.append(np.mean(np.array(crossval_res_kol),axis = 0))\n",
        "        finish= time.time()\n",
        "\n",
        "        print(\"Running time:\",finish-start)\n",
        "    print(\"Avergae result of 10 repeats: \",np.mean(np.array(all_rp_res),axis = 0))"
      ],
      "metadata": {
        "id": "K5BGYiAEnMJp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2312c845-27cd-4369-c6c0-84c5ccd9244e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p_bernoulli:  None\n",
            "augmentaiton:  False aug_factor:  2 num_neighbs:  5 lim4sim:  2\n",
            "use_dropout:  False \n",
            "\n",
            "========================\n",
            "Result of repeat  0 :\n",
            "[0.48571429 0.85       0.15      ]\n",
            "Running time: 18.043320655822754\n",
            "========================\n",
            "Result of repeat  1 :\n",
            "[0.43571429 0.5        0.4       ]\n",
            "Running time: 18.319801568984985\n",
            "========================\n",
            "Result of repeat  2 :\n",
            "[0.53928571 0.25       0.8       ]\n",
            "Running time: 21.053426504135132\n",
            "========================\n",
            "Result of repeat  3 :\n",
            "[0.46071429 0.5        0.45      ]\n",
            "Running time: 23.02398657798767\n",
            "========================\n",
            "Result of repeat  4 :\n",
            "[0.48571429 0.4        0.6       ]\n",
            "Running time: 18.065003633499146\n",
            "========================\n",
            "Result of repeat  5 :\n",
            "[0.51428571 0.2        0.8       ]\n",
            "Running time: 20.66780686378479\n",
            "========================\n",
            "Result of repeat  6 :\n",
            "[0.48571429 0.65       0.35      ]\n",
            "Running time: 19.096226453781128\n",
            "========================\n",
            "Result of repeat  7 :\n",
            "[0.51428571 0.         1.        ]\n",
            "Running time: 19.8189857006073\n",
            "========================\n",
            "Result of repeat  8 :\n",
            "[0.56428571 0.7        0.4       ]\n",
            "Running time: 18.31317710876465\n",
            "========================\n",
            "Result of repeat  9 :\n",
            "[0.56785714 0.46666667 0.65      ]\n",
            "Running time: 18.199400663375854\n",
            "Avergae result of 10 repeats:  [0.50535714 0.45166667 0.56      ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation of two model:**\\\n",
        "The prediction accuracy of model 'Mixup+VAE+DNN' is around 69%, while performance of the 'Mixup+VAE+DNN' is relatively poor(around 50%).\\\n",
        "Limited by the sample size, the prediction accuracy of the model seems to be difficult to improve.\\\n",
        "The performance of two model on some sites improved, but perform worse on the others.\\\n",
        "The accuracy of multiple prediction results on one site is also unstable.\n",
        "\n",
        "**Future direction and further improvement:**\n",
        "1. find more data\n",
        "2. develop novel feature extraction method\n",
        "3. optimize the model (AE and DNN)"
      ],
      "metadata": {
        "id": "HvLwnwgnq6Y-"
      }
    }
  ]
}